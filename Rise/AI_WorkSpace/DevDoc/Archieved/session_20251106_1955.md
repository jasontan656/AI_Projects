- 2025-11-06 22:20 用户提出：希望后端在 MongoDB 不可用时引入 Redis 排队+本地文件兜底的持久化机制，实现数据最终写回 Mongo，提升健壮性。

## 2025-11-07 06:45 追加记录

### 用户意图
- 统一所有入口（HTTP、Telegram 等）必须先写入 Redis 队列，后台 Worker 再处理，确保幂等与重试策略一致。
- 需要立即开始准备迁移，避免后续大规模流量来临前措手不及。
- 当前尚未大规模使用的入口可以阶段性保留，但要声明风险与后续计划。

### 仓库上下文
- `src/interface_entry/http/workflows/routes.py`：`POST /api/workflows/apply` 已构造 `TaskEnvelope` 入队，可选等待 Worker 结果，说明 HTTP 工作流入口已走 Redis（约 145 行）。
- `src/interface_entry/http/dependencies.py`：`get_task_runtime()` 在 FastAPI lifespan 中初始化 `TaskRuntime` 并启动 Worker + RetryScheduler，同时暴露 `get_task_submitter` 等依赖供路由复用（约 171、226 行）。
- `src/foundational_service/persist/redis_queue.py`/`worker.py`：实现 Redis Streams 读写、`XAUTOCLAIM`、重试/挂起、`WorkflowTaskProcessor` 执行 orchestrator 并写 Mongo。
- Telegram 入口（`src/business_service/conversation/service.py`）仍直接 `WorkflowOrchestrator.execute()`，尚未复用队列，是“未统一入口”的主要缺口。

### 技术栈
- FastAPI 0.111.x + Starlette 中间件；接口层统一通过 `ApiResponse` 包装。
- Redis 5.x（python 客户端 `redis==5.0.4`），以 Streams + Consumer Group 实现任务队列。
- Mongo：同步 `pymongo` + 异步 `motor`，当前 Worker 仍用同步客户端。

### 检索结果
- context7 `/redis/redis-doc`（streams.md）示例涵盖 `XADD MAXLEN`, `XGROUP CREATE`, `XREADGROUP`, `XINFO STREAM` 等，验证我们当前指令选型。
- exa: 2025-07-21 Dev.to《Async Job Queues with Redis Streams + asyncio》展示轻量 producer/worker 结构；2025-04 MoneyForward 文章强调可见性超时和自动重试机制。
- （如需更多 FastAPI 官方推荐，可在后续补充 web 搜索记录。）

### 架构发现
- HTTP 工作流 API 已按目标改造，但 Telegram、内部脚本等入口仍未经过 Redis 队列，导致幂等策略、失败反馈不统一。
- `TaskRuntime` 仅在 FastAPI lifespan 内启动；若 Telegram bot 作为独立进程运行，需要额外的 worker bootstrap，否则队列无消费者。
- 重试延迟目前固定于 `WorkflowTaskProcessor.retry_delay()` 的预设曲线（15s→180s），缺少按任务类型调优。
- `TaskEnvelope.context.idempotencyKey` 仅在 HTTP 请求中可选，Telegram 入口尚无同等机制。

### 待办/风险
- 评估 Telegram inbound 与 Redis 队列的衔接方式，并规划灰度迁移（如镜像入队）。
- 统一 `TaskEnvelope.context` 字段，明确 `chat_id`、`user`、`traceId` 的来源，避免入口差异。
- 针对“暂未大规模使用”的入口，明确何时强制切换，并提供 fallback/观察指标。

## 2025-11-07 06:55 文档更新
- 新建 `AI_WorkSpace/DevDoc/On/session_20251107_0645_unified_entry.md`，收敛所有入口入 Redis 的改造方案。
- 文档包含 TaskEnvelope 必填字段、HTTP/Telegram 适配步骤、独立 Worker 启动策略，以及成功路径、失败模式、GIVEN/WHEN/THEN 验收条目。
- 明确防御要点：Redis 不可用需即时失败、wait 超时需返回 202、挂起队列必须透出来源 channel 并供运维恢复。 

## 2025-11-07 07:15 用户问答
- Intent：确认“是否在自行造 Worker 轮子”以及是否应立即切换到 RabbitMQ 作为 Redis 的最终兜底层；若可行，希望直接启用 RabbitMQ（凭 .env 连接信息）解决当前 Worker 可靠性问题。
- Repo Context：当前 Worker 基于 `src/foundational_service/persist/redis_queue.py` + `worker.py`，Redis Streams 为唯一排队媒介；Telegram 入口刚改造为 Redis 入队，尚无 RabbitMQ 集成层。
- Stack Research：RabbitMQ 官方 Reliability Guide（`https://www.rabbitmq.com/docs/reliability`）强调 quorum queue/stream 复制可带来磁盘持久化；近期社区讨论（SecOps® 2025-11-02）对 Redis visibility timeout、消息 retention 限制；InfoQ 2024-01 案例说明 RabbitMQ + Redis 组合用于「通知+缓存」。
- Architecture Findings：
  * 目前 Worker 仅消费 Redis Stream；若 Redis 故障将导致任务阻塞，但不会落地到磁盘。RabbitMQ 作为「兜底」需定义转储流程（例如 `XADD`→`fanout exchange` → durable queue）。
  * 若引入 RabbitMQ，需要新的 Connector（Redis pending → RabbitMQ durable queue）或直接入口写双通道，否则会形成双写一致性问题。
  * Telegram 入口必须“立即入 Redis”仍满足；RabbitMQ 层可以在 Worker 内部作为二级重试/持久存储，不影响入口策略。
- Defensive Considerations：
  * Redis→RabbitMQ 复制需确保幂等（taskId reuse），防止 RabbitMQ 重放后导致 Mongo upsert 冲突。
  * 如果 Worker 也改到 RabbitMQ，需管理两个消费者组的 back-pressure 与监控（Redis `queue:retry` 与 RabbitMQ `quorum queue`）。
  * 在 .env 中配置的 RabbitMQ URL 应通过专用连接池（pika/aiormq）使用，并启用 publisher confirms + durable/quorum queue。

## 2025-11-07 07:25 新文档
- 需求：用户要求“自研 Worker + RabbitMQ 兜底”同时落地，并形成书面方案。
- 新增 DevDoc：`AI_WorkSpace/DevDoc/On/session_20251107_0715_worker_rabbitmq.md`，涵盖拓扑、组件、流程、失败模式、GIVEN/WHEN/THEN。
- 搜索记录：
  * context7(`/rabbitmq/rabbitmq-website`, topic=quorum queues durable behavior) → 指出 quorum queue 声明、策略键、`x-queue-type=quorum`、publisher confirm 等要点。
  * exa("Redis primary queue with RabbitMQ durable fallback architecture 2025") → 2025-02-06 文章《Distributed Processing via Redis Streams...》说明 Redis Streams 作为主队列 + fallback 机制的设计经验。
- 架构要点：Redis 继续作为低延迟入口，新增 Stream Mirror Worker 镜像到 RabbitMQ quorum queue，Rabbit Rehydrator 负责灾备回灌；行文中列出成功路径、失败模式、防御策略与实施步骤。
