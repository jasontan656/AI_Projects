用 gpt‑4o‑mini 作为“轻量多轮执行器”，通过 Responses
  API 的 store:true 与 previous_response_id 把每步产出的“结果片段”显式保存和续接；同时用
  Prompt Caching 给稳定前缀做折扣提速，二者配合，就能把一次复杂推理拆成多步、在多轮里完成并
  且控成本。下面把“能/不能做”的边界与最佳实践说清楚（均已核验官方文档）。

  - 能做什么
      - gpt‑4o‑mini 支持 Responses API 与内建工具，可
  以多轮续接对象来“拼装”一次完整任务；store:true 会把本轮的输入/输出项持久化，下一轮用
  previous_response_id 继续。(openai.com (https://openai.com/index/new-tools-and-features-
  in-the-responses-api/?utm_source=openai))
      - Prompt Caching 对 gpt‑4o/gpt‑4o‑mini 自动生效：复用≥1024 token 的公共前缀会拿到输入
  半价与更快前缀处理；命中量在 usage.prompt_tokens_details.cached_tokens 字段可见；典型 5–10
  分钟失效、最迟 1 小时清除。用来降本，不负责存状态。(openai.com (https://openai.com/index/
  api-prompt-caching/?utm_source=openai))
  - 不要误解的点（避免把两种机制混成“同一个 cache”）
      - Responses 的 store/previous_response_id 是“可控的持久化对象链”，你可以按步骤写入、续
  接、删除；适合保存你说的“中间态结果/提要/决策”。Prompt Caching 仅是“最长公共前缀折扣”，不
  能精确 append 或清除某条。(openai.com (https://openai.com/index/new-tools-and-features-in-
  i18n 模板与短信体约束，尽量不改动以维持命中。(openai.com (https://openai.com/index/api-
  prompt-caching/?utm_source=openai))
      - 每步调用（走 Responses store）：
          1. responses.create(store:true, input_items=[“当前子任务指令+最小变量”, “必要的上
  步结果片段ID/内容”])；
          2. 取得 response_id；下轮用 previous_response_id=该 id 续接，或者把上一轮的“结果片
  段”再作为 input_items 显式传回；
          3. 当某片段过期/无用时，删除对应 response 对象，避免状态污染与成本滚雪
  球。(openai.com (https://openai.com/zh-Hant/index/new-tools-for-building-agents/?
  utm_source=openai))
  - 成本与稳定性要点
      - 让“稳定大前缀”≥1024 tokens 且尽量不动，命中 Prompt Caching 拿输入半价；“中间态结果
  片段”用 store:true 承载；“易变会话态”（如 strikes/冻结窗口/隐私槽位）仍放本地或 Redis，
  避免对远端对象形成强耦合。(openai.com (https://openai.com/index/api-prompt-caching/?
  utm_source=openai))
      - 监控：记录 cached_tokens 命中率、每步 tokens/cost、previous_response_id 续接
  成功率，便于回归与调优。(openai.com (https://openai.com/index/api-prompt-caching/?
  utm_source=openai))

  一句话结论
  - 用 gpt‑4o‑mini + Responses store:true/previous_response_id 管“中间态结果片段”，再配
  Prompt Caching 管“稳定前缀降本”，按步骤把一次复杂推理拆成多轮，是完全可行、工程上稳妥的做
  法；只要别把 Prompt Caching 当状态库即可。上述能力与边界均来自 OpenAI 2025 年官方文档与公
  告。(openai.com (https://openai.com/index/api-prompt-caching/?utm_source=openai))


  

```python
Document mentioned word means : LLM cache. has nothing to do with local status.


User says hi.   >    langdetect   and send language code  > **MarianMT (Helsinki-NLP)**  translation model run local to translate user words to english if user prompt is not english.  > 4o mini reasonging starts with specific mutilple rounds communication designed using openAI LLM cache token function. > send final asnwer to **MarianMT (Helsinki-NLP)** for translation to user original language. > sends back user structured reply with translated content. 

 Chain of thoughts (recursive reasoning),and When a tool call is triggered, you must wait for tool response before continuing reasoning.:

 1.  Store the folloing to cache for place holder filling and instruction following.

     system instruction :
       " You're a cute service secretary from 4 ways group. Your name is Ms. Kobe. Your tone remain human interactive like one satence reply for chipchatting or professionally introducing step by step guide for goverment agency serives inquiry which is focusing on helping user to understand goverment services and how to process them. If user requests. You will as well help user to collect their information and use tool to generate application forms, letters, affidavits, information needed to be collected are other set of documents like photo of IDs, former approval orders, receipts but not limited due to there are alot of service requires different set of documents as checklist and information of application."
     
     User prompt:

     User chat context summery:    <- this is the summery of current chat updated from time to time by you.  this summery should be done as {user prompt summerized : "str" ,timestamp assistant reply :"str" timestamp.} example : 
     
         1. user prompt : " summery : user wanted to know about visa extension price"  assistant reply :"price advised in detail" MM/DD/YEAR and time up to sec.  
         2. user prompt : " summery : user asked if we offer serivce to process for them"  assistant reply :"yes and started assist user with detailed check list to collect info" MM/DD/YEAR and time up to sec.  
         3. user prompt : " summery : provided some info"  assistant reply :"request for missing info" MM/DD/YEAR and time up to sec.  
         4. user prompt : " summery : provided more info"  assistant reply :"requested for uploading of documents for verification" MM/DD/YEAR and time up to sec.  

       chat context summery must be always summrized by you at the end of current reasoning and instruct call redis to apend in current chat session chat context summery. no overwrite until chat context summery rolls off from 20th record and will start giving up the 1st record to override.this function is going to be stored in redis and built to keep conversation on track. as an short memory for the current chat and it doesnt go off. stored in redis for live use and stored in mongodb for long term memory.(mongodb record exceeds 20th,its full record) and when used for chain of thoughts, always include full 20th max with prompt sent. actual chat history must be stored in mongodb as well for full user chat history trace.Chat will be stored in 1 collection and seperated by : 
           
           user id:
           timestamp :
           session id : 
           {
            chat_type :
            {
               one on one chat / group chat :
               {
                  user id :
                  {
                     user prompt :{"str"}
                     assistant reply:{"str"}
                  }
               }
            }
           }

     Judgements to be filled before making any conclusion:

       "inquiry" : bool  
       judgement_v1 :
         "inquiry" : bool
         "assistantReply" : "string | null"
         "nextStep" : "agency_detect_v1" when inquiry == true, otherwise "session_end"

       agency_detect_v1 :
         "judgements.agencyDetected" : ["agencyId", ...]
         "judgements.agencyInfo" : {
            list of agency info loaded from KnowledgeBase_index.yaml for cache sync
         }
         "judgements.agencyCount" : integer that matches agencyDetected length
         "judgements.complexity" : "low" | "high"
         "nextStep" : "category_select_v1" (low) | "semantic_analysis_v1" (high) | "session_end"

       category_select_v1 (only when complexity == low) :
         "categorySelection.candidates" : {
           "category_key" : {"value" : "description", "score" : 0.75}
         }
         offline script may still project this into legacy "key_defination" for backward compatibility

       semantic_analysis_v1 (only when complexity == high) :
         "semanticAnalysis.agencies" : [
           {"agencyId" : "string", "agencyName" : "string", "categorySelection" : {...}}
         ]
         "semanticAnalysis.primaryAgency" : {"agencyId" : "string", "reason" : "string"}

       service_select_v1 :
         "userChatContextSummary" : [
            {
              "userPromptSummary" : "string",
              "assistantReplySummary" : "string | null",
              "timestamp" : "YYYY-MM-DDTHH:MM:SSZ"
            }
         ]
         "serviceSelection" : {
            "serviceKey" : "string",
            "name" : "string",
            "path" : "string",
            "matchedField" : "key | name | overview | applicability_summary",
            "score" : 0.00
         }
         "template.placeholders" : {
            "service_name" : "{service_name}",
            "...dynamic..." : "{...dynamic...}"
         }
         "template.rules" : "Describe how placeholders should be arranged"

       multi_agency_service_answer_v1 (only when orchestrator branches after service_select_v1) :
         "userChatContextSummary" : [
            {
              "userPromptSummary" : "string",
              "assistantReplySummary" : "string",
              "timestamp" : "YYYY-MM-DDTHH:MM:SSZ"
            }
         ]
         "assistantReply" : "string"
         "sourcesUsed" : [{"agencyId":"string","serviceKey":"string","path":"string"}]
         "nextStep" : "session_end"

       Additional state required by translation / analytics layers:
         "userChatContextSummary" : latest ≤20 entries mirrored to Redis + Mongo
         "intent_keywords" : ["multiple words"]
         "service_index" : {
            "{agency_id_upper}_index.yaml": {
               "id": "string",
               "name": "string",
               "path": "KnowledgeBase/...yaml",
               "aliases": ["alias1","alias2"],
               "overview": "short summary",
               "applicability_summary": "when to use"
            }
         }
         "addtional_info": "str"
         "user_language" : "str"

2. Execution Tasks detailed guide. Execute step by step :
    
    Determinate weather the current user prompt sounds like a goverment agency service inquiry or just saying greetings or might as well being silly. we dont mind client wants to know about us. but we dont want the client keep on wasting our token.so we need to know if this user really needed something or block the silly ones by freezing the chat service. after you understand the user prompt,you shall return "if inquiry?"  true / false.
     
    then fill the judgements to be filled
      if
       inquiry : False. 
         return the following:{
                to user : answer user directly as your role set.
                to backend : "inquiry : false", counted as 1 violation to be sent to (guard agent (offline python script code)) to mark this user 1 violation. continues 5 violations will result assistant responde freeze, guard agent will block user prompt from reaching the procedure and taking over to record only user prompt to mongodb and assistant reply to mongo db as chat history and always return user the mechine template response " None inquiry intent detected, Cooling down {time count down 15mins and display remining time count}"
         } 
      Round end
         Clear cache

    guard agent:
    - guard agent only checks number count of certain chat user if it has reached 5. if yes, lock that chat user to be routed always to guard agent and start to reply mechine generated messgae "No inquiry intent detected, system has been locked for {time count down max 15mins and display time left each time interacts with user message}" 
      
      else load the next part of prompt of following
      
      inquiry : True
          读取 Kobe/KnowledgeBase/KnowledgeBase_index.yaml 中的机构清单并写入缓存：
             {agency_id}{name}{path}{description}  <- 与当前 YAML 字段保持一致
          determinate which agency is related to user concern and fill "judgements.agencyDetected".
         
         determinate if agency related is more than 1.
            if agency detected number count == 1, set judgements complexity : low 并执行以下步骤：
               - 运行 category_select_v1：加载 KnowledgeBase/{agency_id_upper}/{agency_id_upper}_dictionary.yaml，把实际存在的 key/description 写入 categorySelection.candidates（score ≥ 0.50），离线脚本可同步生成 legacy "key_defination"。
               - 运行 service_select_v1：结合 KnowledgeBase/{agency_id_upper}/{agency_id_upper}_index.yaml 的 {id, name, path, aliases, overview, applicability_summary}，模型输出 userChatContextSummary + serviceSelection + template.placeholders。离线脚本依据 placeholders 与 YAML 数据组装最终回复，并把最新 summary 追加到 Redis 对应会话、同步到 MongoDB 持久化存档（控制在 20 条内，滚动淘汰）。
               - 低复杂度路径默认 nextStep = session_end。完成后，离线脚本写回最新 state、发送答复、清空缓存。
            
      else 
      load the next part of prompt of following:

            if agency detected number count > 1, set judgements complexity : high 并执行以下步骤：
               - 运行 semantic_analysis_v1：对每个机构单独做语义匹配，写入 semanticAnalysis.agencies[]，并选出 semanticAnalysis.primaryAgency；离线脚本据此决定需要缓存的 index / service YAML。
               - 运行 service_select_v1：按 primary agency 或多机构组合挑选 serviceSelection、生成 template.placeholders，并补充 userChatContextSummary；同时可能把 nextStep 设置为 multi_agency_service_answer_v1。
               - 若 nextStep == multi_agency_service_answer_v1，则继续执行：汇总多机构服务，输出 assistantReply + sourcesUsed + userChatContextSummary，形成最终答复骨架，并由离线脚本负责 Redis/Mongo 同步。
               - 整个高复杂度流程中，离线脚本负责按需读取 KnowledgeBase/{agency_id_upper}/{service_id}/{service_id}.yaml，填充 "service_index"、"addtional_info"、"intent_keywords"，并在最后填充 placeholders。
         Round end
          Clear cache
               
                
 
3. Offline Orchestrator Responsibilities (store:true pipeline):

   The runtime outside of the LLM is responsible for orchestrating stage calls, persisting intermediate
   state, and maintaining the chat context summary for Redis + MongoDB.

   1) Load or bootstrap `cached_state.json` with `session_id`, `response_id`, `nextStep` (defaults to `judgement_v1`),
      plus any static metadata (user locale, guard status, etc.).
   2) While `nextStep != "session_end"`:
        a. Read `stage_manifest.yaml` → locate prompt file for current stage.
        b. Assemble invocation payload:
           - Base system prompt (`prompt_base_system.md`, already cached via store:true).
           - Stage-specific command prompt (e.g., `prompt_stage3-2_templatefill.md`).
           - User block composed of:
               • `{{input.user_prompt}}` = latest turn from end user.
               • `{{input.chat_context_summary}}` = up to 20 entries from persisted summary.
               • `{{input.cached_state}}` = current JSON state (serialized).
               • `{{input.runtime_directive}}` = optional instruction from guard agent/ops.
               • Wherever required: `{{input.KnowledgeBase_Dictionary}}`, `{{input.service_index}}`,
                 or other YAML-derived inputs loaded from KnowledgeBase.
        c. Call Responses API with `store:true`, `previous_response_id` = last successful stage response if reuse is desired.
        d. Validate model output against `stage_runtime_contract.md` + prompt-specific rules:
               • JSON parseable, mandatory fields present.
               • `session_id` continuity, scores precision, `nextStep` value matches manifest transition.
               • `userChatContextSummary` exists for stages that require it (`service_select_v1`, `multi_agency_service_answer_v1`).
        e. On validation failure:
               • Inspect `telemetry.notes` (if present), log reason.
               • Decide whether to re-issue the same stage (up to N retries) or escalate to guard agent/manual review.
        f. State merge:
               • Update `cached_state.<stage_id>` with fields specified in `stage_runtime_contract.md`.
               • Update `cached_state.nextStep` with stage response `nextStep`.
               • Append or overwrite supporting attributes (`intent_keywords`, `service_index`, `addtional_info`, etc.).
        g. Chat summary maintenance:
               • Extract latest entry from `userChatContextSummary` (if provided) and append to Redis list keyed by user/session.
               • Keep Redis list length ≤20 (pop oldest when necessary).
               • Upsert same entry into MongoDB history collection with user id, session id, timestamp.
        h. Knowledge base enrichment:
               • When stage requires dictionary/service index, load corresponding YAML (e.g., `KnowledgeBase/BI/BI_index.yaml`)
                 and inject the relevant fragments into `cached_state` before invoking the next stage.
        i. Guard agent hooks:
               • After `judgement_v1`, increment violation counter if `inquiry=false`.
               • If guard threshold reached, short-circuit `nextStep` to `session_end` and route future user turns to guard handler.

   3) After loop exits (`nextStep == "session_end"`):
        • Fetch the final `assistantReply` (from `service_select_v1` template rendering pipeline or
          `multi_agency_service_answer_v1`) and deliver to user.
        • Persist final `cached_state` snapshot, Redis summary, and Mongo chat record.
        • Optionally ensure any response objects stored via `store:true` are tagged for retention/cleanup per policy.

   This offline workflow is the glue that turns per-stage command outputs into a coherent, stateful dialogue,
   guaranteeing that Redis/Mongo remain authoritative for chat summaries, while `cached_state.json` stays aligned
   with the orchestrator’s branching logic.

4. Data store initialization & warm-up

   To avoid ad-hoc collection creation at runtime, run the helper once per environment:

   ```bash
   python SharedUtility/scripts/init_data_stores.py
   ```

   - 环境变量：脚本读取 `.env` 中的 `MONGODB_URI`、`MONGODB_DATABASE`、`REDIS_URL`。
     可用 `CHAT_SUMMARY_COLLECTION` 覆盖默认集合名 `chat_summaries`。
   - MongoDB：脚本创建（或确认存在）集合 `<database>.<collection>`，并建立索引
     `idx_chat_id_unique`（唯一 `chat_id`）与 `idx_updated_at`（按更新时间排序）。
     文档结构建议：

     ```json
     {
       "chat_id": "<telegram chat id or composed key>",
       "entries": [
         {
           "userPromptSummary": "string",
           "assistantReplySummary": "string | null",
           "timestamp": "2025-10-31T15:20:45Z"
         }
       ],
       "updated_at": "2025-10-31T15:20:45Z"
     }
     ```

     离线脚本每次 upsert：先弹出现有记录（若超 20 条），再写入新数组，并更新 `updated_at`。

   - Redis：脚本仅做 `PING` 验证，无需创建集合。约定以
     `chat:{chat_id}:summary` 为键名，值类型为 list，内容与 Mongo `entries` 同构。

   运行时准备与清理流程：
   1) 当某个 chat 在 1 小时（可配置）内首次触发流程：
        - 从 Mongo 读取对应文档（若存在），取最近 20 条写入 Redis list，同时设定 `EXPIRE = 3600` 秒。
   2) 每次追加新的 `userChatContextSummary`：
        - `LPUSH` 写入 redis（保持最新在前）。
        - `LTRIM 0 19` 保持 20 条上限。
        - 立即 `EXPIRE` 重置倒计时，确保活跃对话不过早过期。
        - 将列表同步回 Mongo（见上方 upsert 逻辑）。
   3) 若 1 小时内无互动，redis key 自然过期；下次对话重新从 Mongo 载入，避免在项目启动时加载所有历史。

   Guard agent 与其它后台任务可以重复使用同一 redis/mongo 连接（通过共享的连接池封装），确保
   读写策略一致。

5. Path resolution & contract reuse

   - `stage_manifest.yaml` 里的 `path` 均为相对路径（相对于 `OpenaiAgents/CS_AgentContract/`）；
     orchestrator 加载 prompt 文件时必须基于仓库根路径拼接，禁止把开发机本地的绝对路径写进状态。
   - prompt 文档引用的 YAML 资料（例如 `KnowledgeBase/BI/BI_index.yaml`）也一律使用仓库内相对路径。
     运行时设置 `PROJECT_ROOT = Path(__file__).resolve().parents[2]`，后续统一使用
     `PROJECT_ROOT / relative_path` 定位文件。
   - `cached_state.json`、日志、临时输出等都应放在工程内的既定目录，通过配置或环境变量声明相对路径，
     既方便迁移，又避免把 machine-specific 信息写入缓存。
   - 对于长期复用的 prompt/contract，可以在启动时批量读取到内存缓存，但缓存键务必使用相对路径或
     文件哈希，避免因路径差异导致缓存 miss。

   只要 orchestrator 与离线脚本遵循上述约定，就能确保 `01.md` 中描述的流程在不同环境下均可复用，
   而不会被绝对路径耦死。

6. Per-chat cached state layout

   多个用户/群聊会并行使用 UnifiedCS，每个对话都需要独立维护一份 `cached_state.json`：

   - 推荐目录结构：

     ```
     OpenaiAgents/CS_AgentContract/runtime_state/
       ├── chat_<chat_id>/cached_state.json
       ├── chat_<chat_id>/history.log        # 可选：阶段执行日志
       └── ...
     ```

     由 orchestrator 在启动时根据 `chat_id` 自动创建目录，确保不同会话互不干扰。

   - 会话流程：
       1. 进入新对话 → 判断 `runtime_state/chat_<chat_id>/cached_state.json` 是否存在。
          - 存在：加载继续执行。
          - 不存在：调用 bootstrap 逻辑生成初始化 state（`nextStep = judgement_v1`），立即写入该目录。
       2. 每轮 stage 完成后，覆写该文件，保证磁盘总是保存最新状态。
       3. 会话结束（`nextStep = session_end`）后：
          - 若需要复盘或审计，可以保留文件或压缩归档。
          - 若无需持久化，可在发完最终回复后删除 `cached_state.json` 与相关暂存文件。

   - 统一入口：建议封装一个 `StateStore` 工具类，负责 `load_state(chat_id)`、`save_state(chat_id, state)`、
     `cleanup(chat_id)` 等操作，内部始终使用上述相对路径，避免业务代码散落硬编码。

   - 与 Redis/Mongo 的配合：`cached_state.json` 只承载阶段状态；`userChatContextSummary` 仍以 Redis 为主、
     Mongo 为备份。不要把聊天摘要塞进 state 文件里，以免重复维护。

   这样设计可以让 orchestrator 在多会话环境下保持清晰的文件边界，同时方便后续排查某个 chat 执行链
   的完整状态。
