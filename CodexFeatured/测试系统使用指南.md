# 测试系统使用指南

**版本**：2.0
**更新日期**：2025-10-11

---

## 快速开始

### 完整的5步工作流

```
Step 1: 生成测试场景文档（必需）
  ↓
codex -p .codex/prompts/TestScenarioAnalysis_V2.md

  ↓
Step 2: 精化测试场景（可选）
  ↓
codex -p .codex/prompts/TestScenarioSpecify_V2.md -a "增加场景：..."

  ↓
Step 3: 生成测试计划（必需）
  ↓
codex -p .codex/prompts/TestPlanGeneration_V2.md

  ↓
Step 4: 生成技术决策（必需）
  ↓
codex -p .codex/prompts/TestTechDecisions_V2.md

  ↓
Step 5: 执行测试与调试（必需）
  ↓
codex -p .codex/prompts/TestExecuteAndDebug_V2.md

  ↓
完成！查看报告
```

---

## 详细使用说明

### Step 1：TestScenarioAnalysis（测试场景分析）

**目的**：深度理解项目，从7个维度设计测试场景

**运行命令**：
```bash
codex -p .codex/prompts/TestScenarioAnalysis_V2.md
```

**工作原理**：
1. 自动定位最新的开发项目（DevPlans目录）
2. 读取需求文档、开发计划、技术决策
3. 扫描项目代码（index.yaml + 实际代码）
4. 从7个维度发散性思考测试场景：
   - 功能覆盖
   - 数据多样性
   - 并发与性能
   - 配置分支
   - 异常恢复
   - 依赖服务
   - 真实场景
5. 为每个场景设计随机化策略
6. 生成50-100个测试场景

**输出**：
- `Kobe/SimulationTest/{MODULE_NAME}_testscenarios.md`
- 包含所有场景的详细定义

**人工审核**：
- 检查场景是否覆盖所有核心功能
- 检查随机化策略是否合理
- 检查验收标准是否可量化

---

### Step 2：TestScenarioSpecify（测试场景精化，可选）

**目的**：根据人工审核结果，调整和优化测试场景

**运行命令**：
```bash
codex -p .codex/prompts/TestScenarioSpecify_V2.md -a "你的补充需求"
```

**使用场景**：
1. **增加新场景**：
   ```bash
   codex -p .codex/prompts/TestScenarioSpecify_V2.md -a "增加场景：测试100并发用户"
   ```

2. **调整优先级**：
   ```bash
   codex -p .codex/prompts/TestScenarioSpecify_V2.md -a "将Scenario-1.3优先级从P1改为P0"
   ```

3. **修改验收标准**：
   ```bash
   codex -p .codex/prompts/TestScenarioSpecify_V2.md -a "放宽Scenario-1.2的响应时间从5分钟到10分钟"
   ```

4. **增强随机化**：
   ```bash
   codex -p .codex/prompts/TestScenarioSpecify_V2.md -a "增强Scenario-2.1的随机化策略"
   ```

**工作原理**：
1. 读取现有测试场景文档
2. 分析用户输入的影响范围
3. 智能更新受影响的章节
4. 标注修改内容（`<!-- MODIFIED: ... -->`）
5. 添加修改历史

**输出**：
- 更新后的测试场景文档（覆盖原文件）

**可迭代**：
- 可以多次运行此步骤，逐步完善场景

---

### Step 3：TestPlanGeneration（测试计划生成）

**目的**：设计测试代码的组织方案

**运行命令**：
```bash
codex -p .codex/prompts/TestPlanGeneration_V2.md
```

**工作原理**：
1. 读取测试场景文档
2. 设计完整的目录结构
3. 设计数据准备方案（生成器 + 固定样本）
4. 设计环境搭建方案（依赖服务检查）
5. 设计执行顺序（依赖关系 + 拓扑排序）
6. 设计结果收集方案（日志 + 报告）

**输出**：
- `Kobe/SimulationTest/{MODULE_NAME}_testplan.md`
- 包含完整的测试组织设计

**人工审核**：
- 检查目录结构是否合理
- 检查执行顺序是否正确
- 检查环境搭建方案是否完整

---

### Step 4：TestTechDecisions（测试技术选型）

**目的**：定义测试实现的技术细节

**运行命令**：
```bash
codex -p .codex/prompts/TestTechDecisions_V2.md
```

**工作原理**：
1. 读取测试场景和测试计划
2. 确定测试框架和依赖（pytest生态）
3. 定义随机化技术实现（完整代码）
4. 定义并发测试技术实现
5. 定义模拟（Mock）技术实现
6. 定义性能监控技术实现
7. 生成requirements.txt和pytest.ini

**输出**：
- `Kobe/SimulationTest/{MODULE_NAME}_testtech.md`
- 包含所有技术实现的完整代码

**人工审核**：
- 检查依赖版本是否合适
- 检查随机化代码是否可执行
- 检查配置是否完整

---

### Step 5：TestExecuteAndDebug（测试执行与调试）

**目的**：实现测试组件，执行测试，智能调试

**运行命令**：
```bash
codex -p .codex/prompts/TestExecuteAndDebug_V2.md
```

**工作原理**：

**阶段1：测试实现**
1. 读取所有测试文档
2. 创建完整的测试组件目录
3. 生成所有配置文件（requirements.txt, pytest.ini等）
4. 生成所有工具模块（7个工具文件）
5. 生成conftest.py（所有fixtures）
6. 生成测试用例文件（7个维度）
7. 生成run_tests.py（测试执行器）

**阶段2：执行与调试**
1. 安装依赖
2. 检查环境（所有依赖服务）
3. 运行P0场景（核心场景，必须100%通过）
4. 运行P1场景（重要场景）
5. 如果失败，进入智能Debugging模式

**Debugging模式**（关键！）：
- **维度1：理解项目背景**（读需求文档）
- **维度2：检查基础设施**（服务是否启动？）
- **维度3：检查配置和环境**（配置是否正确？）
- **维度4：检查代码实现**（功能是否实现？）
- **维度5：检查测试本身**（测试是否合理？）

**输出**：
- 完整的测试组件目录 `Kobe/SimulationTest/{MODULE_NAME}_testplan/`
- 测试报告 `results/report.html` 和 `results/report.json`
- Debugging报告（如果有失败场景）

---

## 典型工作流示例

### 场景1：为新开发的功能生成测试

```bash
# 前提：已经完成了开发工作流（5步），生成了DevPlans/005_XXX/

# Step 1: 生成测试场景
codex -p .codex/prompts/TestScenarioAnalysis_V2.md
# 输出：SimulationTest/XXX_testscenarios.md

# 人工审核：查看testscenarios.md，检查场景是否全面

# Step 2: 精化场景（如果需要）
codex -p .codex/prompts/TestScenarioSpecify_V2.md -a "增加场景：测试100并发用户"
# 输出：更新后的testscenarios.md

# Step 3: 生成测试计划
codex -p .codex/prompts/TestPlanGeneration_V2.md
# 输出：SimulationTest/XXX_testplan.md

# 人工审核：查看testplan.md，检查组织方案是否合理

# Step 4: 生成技术决策
codex -p .codex/prompts/TestTechDecisions_V2.md
# 输出：SimulationTest/XXX_testtech.md

# 人工审核：查看testtech.md，检查技术选型是否合适

# Step 5: 执行测试
codex -p .codex/prompts/TestExecuteAndDebug_V2.md
# 输出：
# - SimulationTest/XXX_testplan/（完整测试组件）
# - results/report.html（测试报告）

# 查看报告
cd Kobe/SimulationTest/XXX_testplan
open results/report.html
```

### 场景2：只运行测试（测试组件已存在）

如果测试组件已经生成，想重新运行测试：

```bash
cd Kobe/SimulationTest/XXX_testplan

# 检查环境
python run_tests.py --check-only

# 运行P0场景
python run_tests.py --priority p0

# 运行所有场景
pytest test_cases/

# 并行运行
python run_tests.py --parallel 4

# 查看报告
open results/report.html
```

### 场景3：调试失败的测试

如果测试失败：

```bash
# 1. 查看测试报告
open results/report.html

# 2. 查看失败场景的日志
cat logs/by_scenario/scenario_3_2.log

# 3. 使用相同随机种子复现
# （从日志中获取种子，例如123456）
TEST_RANDOM_SEED=123456 pytest test_cases/ -k "scenario_3_2"

# 4. 如果需要AI帮助调试
codex -p .codex/prompts/TestExecuteAndDebug_V2.md
# AI会自动进入Debugging模式，按5个维度诊断问题
```

---

## 核心概念

### 1. 7个测试维度

| 维度 | 目的 | 示例场景 |
|------|------|---------|
| 功能覆盖 | 验证所有功能 | 正常导入、大文件导入、空文件导入 |
| 数据多样性 | 测试各种数据 | 特殊字符、大数据量、边界情况 |
| 并发与性能 | 验证性能 | 10并发、100并发、持续压力 |
| 配置分支 | 测试不同配置 | Redis开/关、Debug模式 |
| 异常恢复 | 测试容错性 | 网络超时、服务重启 |
| 依赖服务 | 测试依赖 | MongoDB宕机、Redis宕机 |
| 真实场景 | 模拟用户使用 | 完整的用户流程 |

### 2. 随机化策略

**为什么需要随机化？**
- 模拟真实用户的不可预测行为
- 增加测试覆盖率
- 发现边界问题

**5种随机化类型**：
1. **数据量随机**：10-20条消息（随机）
2. **顺序随机**：消息顺序随机打乱
3. **延迟随机**：模拟网络延迟0-5秒
4. **失败注入**：10%概率模拟API失败
5. **内容随机**：使用faker生成随机文本

**可复现性**：
- 所有测试使用统一的随机种子
- 种子在测试开始时打印
- 使用相同种子可以精确复现测试

### 3. 智能Debugging

**传统Debugging（不推荐）**：
- 看到错误消息
- 猜测可能的原因
- 反复尝试修改代码
- 死磕单个bug

**智能Debugging（推荐）**：
- 从5个维度系统性诊断
- 理解项目背景和开发意图
- 识别根本原因
- 提供根本性解决方案
- 预防类似问题再次发生

**示例对比**：

```
问题：Celery任务一直pending

传统思路：
→ 检查Celery代码
→ 修改任务代码
→ 还是不行
→ 继续修改
→ 还是不行
→ ...（死磕代码）

智能思路：
→ 维度1：理解背景（需求文档说使用Celery）
→ 维度2：检查基础设施（Celery worker启动了吗？）
→ 发现：没有启动Celery worker！
→ 根本原因：基础设施问题，不是代码bug
→ 解决方案：启动worker，更新启动脚本和文档
→ 预防：在测试前检查Celery状态
```

### 4. 测试组件结构

```
{MODULE_NAME}_testplan/
├── test_cases/          # pytest测试用例（7个文件）
├── test_data/           # 测试数据
│   ├── generators/      # 数据生成器
│   └── fixtures/        # 固定样本
├── utils/               # 工具函数
│   ├── api_client.py    # API调用封装
│   ├── db_client.py     # 数据库客户端
│   ├── service_checker.py  # 服务检查
│   └── performance_monitor.py  # 性能监控
├── results/             # 测试结果（JSON + HTML）
├── logs/                # 日志文件
├── requirements.txt     # 依赖清单
├── pytest.ini           # pytest配置
├── test_config.py       # 测试配置
└── run_tests.py         # 测试执行器
```

**不是简单脚本！**
- 完整的pytest生态
- 结构化的代码组织
- 完善的日志和报告
- 可维护、可扩展

---

## 常见问题

### Q1：为什么测试场景生成需要这么长时间？

A：因为AI在做深度分析：
- 读取所有开发文档
- 扫描项目代码
- 理解实现细节
- 从7个维度发散思考
- 生成50-100个详细场景

这个过程确保测试场景的全面性和准确性。

### Q2：可以跳过某些步骤吗？

A：不建议。5步是逐层细化的：
- Step 1: 场景（What to test?）
- Step 3: 计划（How to organize?）
- Step 4: 技术（How to implement?）
- Step 5: 执行（Do it!）

跳过某步会导致后续步骤缺少必要信息。

唯一可选的是Step 2（精化），如果场景已经完美，可以跳过。

### Q3：测试失败时，AI会帮我修复吗？

A：是的。AI会：
1. 按5个维度系统性诊断
2. 生成详细的Debugging报告
3. 识别根本原因
4. 提供根本性解决方案
5. 自动实施修复并验证

但是，对于某些需要人工判断的问题（如业务逻辑错误），AI会给出建议但不会自动修改。

### Q4：随机化会导致测试不稳定吗？

A：不会，因为：
- 使用统一的随机种子
- 种子在测试开始时打印
- 使用相同种子可以精确复现

如果测试失败，复制种子值，重新运行：
```bash
TEST_RANDOM_SEED=123456 pytest test_cases/
```

### Q5：如何为已有项目补充测试？

A：完全相同的流程！只要你有：
- DevPlans/{COUNT_3D}_{INTENT_TITLE}/DemandDescription.md
- 项目代码

AI就能分析并生成测试。

---

## 最佳实践

### 1. 每次开发完成后立即测试

```
开发工作流（5步）
  ↓
测试工作流（5步）
  ↓
提交代码
```

### 2. 定期运行回归测试

```bash
# 每周运行一次完整测试
cd Kobe/SimulationTest/XXX_testplan
pytest test_cases/
```

### 3. 测试失败时，先看Debugging报告

AI会自动生成详细的诊断报告，先看报告再动手。

### 4. 保留测试组件

测试组件是完整的pytest项目，可以：
- 持续迭代
- 版本控制（Git）
- 在CI/CD中运行

### 5. 合理使用随机化

- P0场景：少量随机化（确保稳定）
- P1/P2场景：中等随机化（增加覆盖）
- P3场景：大量随机化（探索边界）

---

## 工作流对照

| 步骤 | 开发工作流 | 测试工作流 | 时间估算 |
|------|-----------|-----------|---------|
| 1 | DevFuncDemandsWrite | TestScenarioAnalysis | 5-10分钟 |
| 1.5（可选） | DevFuncDemandsSpecify | TestScenarioSpecify | 2-5分钟 |
| 2 | DevPlanGeneration | TestPlanGeneration | 3-5分钟 |
| 3 | TechDecisionsGeneration | TestTechDecisions | 5-8分钟 |
| 4 | DevPipelineGeneration | （合并到Step 5） | - |
| 5 | DevPiplineExcute | TestExecuteAndDebug | 10-30分钟 |
| **总计** | **约30-60分钟** | **约25-60分钟** | **约1-2小时** |

---

**版本**：2.0 | **最后更新**：2025-10-11

